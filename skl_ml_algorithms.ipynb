{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "practical-october",
   "metadata": {},
   "source": [
    "# Algoritmos de machine learning en SciKit-Learn\n",
    "\n",
    "## Notas para elegir un algorimo de machine learning\n",
    "\n",
    "* Recordar que no hay algorimo universal que funcione para todos los casos ni para todos los escenarios de un caso.\n",
    "* Probar varios algoritmos para encontar el mejor, (supongo que hay que probar con cabeza, no usar todos porque sí.)\n",
    "* Tener en cuenta las caracteristicas del dataset.\n",
    "* Entender que la calidad de la predicción depende de la calidad de los datos, (en general todo de pende de los datos iniciales)\n",
    "\n",
    "5 pasos para entrenar un algoritmo de machine learning (segun pml de rashka):\n",
    "\n",
    "1. Selection of features. (\"columnas\")\n",
    "2. Choosing a performance metric. (cantidad de errores/calidad de predicción)??'\n",
    "3. Choosing a classifier and optimization algorithm.\n",
    "4. Evaluating the performance of the model. \n",
    "5. Tuning the algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "adaptive-object",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import datasets\n",
    "import numpy as np\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data[:, [2,3]] #selecciona medidas de petalo, recuerdo que era mas facil, pero: ahora hay 3 clases\n",
    "y = iris.target\n",
    "\n",
    "np.unique(Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "immune-conservation",
   "metadata": {},
   "source": [
    "0=Iris-Setosa, 1=Iris-Versicolor, 2=Iris-Virginica.  \n",
    "Es importante pasar etiquetas a numeros para mayor eficiencia, aquí ya están transformadas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "standing-stick",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "# 30% para pruebas, 70% para entrenamiento, supongo que stratify especifica que lo haga para cada clase\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 1, stratify = y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "sitting-latex",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "# estandarizando X como en el caso de adaline\n",
    "# ahora veo que es recomendable hacerlo antes de usar datos con librerias de ml\n",
    "sc = StandardScaler()\n",
    "sc.fit(X_train)\n",
    "X_train_std = sc.transform(X_train)\n",
    "X_test_std = sc.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "female-pricing",
   "metadata": {},
   "source": [
    "## Perceptrón\n",
    "\n",
    "* implementando el perceptron, rashka vuelve a mencionar que no es recomendable usarlo en la practica  \n",
    "    -> es porque no converge(nunca \"termina\") si los datos no son __perfectamente__ separables linealmente\n",
    "* OvR one vs rest, por lo que veo en internet, parece mejor estrategia que OvA one vs all, habra que invstigar las desventajas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "designed-proposition",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Perceptron(eta0=0.1, max_iter=40, random_state=1)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import Perceptron\n",
    "\n",
    "ppn = Perceptron(max_iter = 40, eta0 = 0.1, random_state = 1)# no hay n_iter pero hay max_iter\n",
    "ppn.fit(X_train_std, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "selected-study",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = ppn.predict(X_test_std) # se realiza la predicción\n",
    "(y_test != y_pred).sum() # parece que usar \"random_state = 1\" en lugar de \"= 0\" disminuye los errores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "relative-record",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.978\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "print('Accuracy: %.3f' % accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "southeast-gospel",
   "metadata": {},
   "source": [
    "## Logistic regression\n",
    "\n",
    "* es un modelo de clasificación, no de regresión.\n",
    "* es de los más usado\n",
    "* puede extenderse con OvR\n",
    "\n",
    "tasa de probabilidad = p/(1-p)\n",
    "\n",
    "logit(p) = log p/(1-p)\n",
    "\n",
    "No me queda clara toda la explicación previa, creo que ví el teorema de bayes(?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "catholic-three",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0+"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
