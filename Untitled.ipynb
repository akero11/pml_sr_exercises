{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "designing-cincinnati",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adjusted-smoke",
   "metadata": {},
   "source": [
    "## AdaLiNe GD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "running-supply",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdaLiNeGD:\n",
    "    def __init__(self, lr = 0.01, n_iter = 50):\n",
    "        self.lr = lr # Taza de aprendizaje\n",
    "        self.n_iter = n_iter\n",
    "\n",
    "    def fit(self, X, y):        \n",
    "        self.w_ = np.zeros(1 + X.shape[1]) # Iniciar con pesos en 0\n",
    "        self.cost_ = []\n",
    "\n",
    "        for i in range(self.n_iter):\n",
    "            output = self.net_input(X) # Recortado debido a que se usa f(x) = x\n",
    "            errors = (y - output) # Calcula el error\n",
    "            self.w_[1:] += self.lr * X.T.dot(errors) # Cambia los pesos, X.T.dot(errors) = derivada parcial?\n",
    "            self.w_[0] += self.lr * errors.sum() # que hace w0?...\n",
    "            cost = (errors**2).sum() / 2.0 # calcula el coste\n",
    "            self.cost_.append(cost) # guarda el coste\n",
    "        return self\n",
    "\n",
    "    def net_input(self, X): # Esto es calcular z\n",
    "        return np.dot(X, self.w_[1:]) + self.w_[0] \n",
    "\n",
    "    def predict(self, X):\n",
    "        return np.where(self.activation(self.net_input(X)) >= 0.0, 1, -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "upper-dimension",
   "metadata": {},
   "source": [
    "### AdaLiNe GD - Notas\n",
    "\n",
    "#### fit\n",
    "\n",
    "* Recorté toda la parte de la función de activación porque entiendo que no influye en nada, segun Rashka, es algo simbolico para, más adelante, cambiarlo por regresion logistica.\n",
    "* Creo que no comprendo lo que hace la parte \"errors = (y - output)\"  \n",
    "    Creo que ya entiendo, es como la otra vez, si el output es diferente a y entonces ahi es cuando se aclcula el error.  \n",
    "    Ya entiendo de dode surge, es parte de la derivada parcial.\n",
    "* en la parte de \"X.T.dot(errors)\", yo entiendo que todo eso es el calculo de la derivada parcial.\n",
    "* Entender como se involucra w0 me cuesta trabajo, creo que tengo que ponerle mas atencion.  \n",
    "    Ahora que lo pienso, w0 es el limite de activación(o algo asi)... <- __Investigar sobre eso__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "approved-lodging",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0+"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
